# -*- coding: utf-8 -*-
"""project_547.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/166_vwpbKw1HVf9ZTbvSNQjuhRTgH_z2H
"""

# Let's install the relevant packages
!pip install diffusers
!pip install gradio -q

# Let's load the libraries
from diffusers import StableDiffusionPipeline
import torch

# Let's import the models
model_id = "stabilityai/stable-diffusion-2"

pipe = StableDiffusionPipeline.from_pretrained(model_id, revision="fp16", torch_dtype=torch.float16)
pipe = pipe.to("cuda") #we push the model to the GPU

prompt = "cat themed"

def text2image(prompt):
  """Generates an image from a text prompt, handling programming-related
     requests with code blocks, if applicable.

  Args:
    prompt: The text description for the desired image.

  Returns:
    PIL.Image: The generated image.
  """

  image = pipe(prompt, height=768, width=768, guidance_scale = 10).images[0]

  image.save("sd_image.png")

  return image

text2image(prompt)

"""Section 3: text to image"""

import gradio as gr

gr.Interface(text2image, gr.Text(), gr.Image(), title = 'Stable Diffusion 2.0 Colab with Gradio UI').launch(share = True, debug = True)

#add a code when it generates

"""Section 3: image to text (label output)"""

import tensorflow as tf #Imports TensorFlow, a powerful library for numerical computation and machine learning.
import numpy as np
from PIL import Image
import gradio as gr

import requests
from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration
import gradio as gr

processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

def predict_image(image):
    raw_image = Image.fromarray(image.astype('uint8'), 'RGB')
    inputs = processor(raw_image, return_tensors="pt")

    out = model.generate(**inputs)
    caption = processor.decode(out[0], skip_special_tokens=True)
    return caption

iface = gr.Interface(fn=predict_image,
                     inputs=gr.Image(),
                     outputs="text",
                     title="BLIP Image Captioning",
                     description="Upload an image to generate a textual description")

iface.launch()

"""Section 4: image to text (recommendtion output)"""

import requests
from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration
import gradio as gr

processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

# Assuming we only identify a few basic scene types for simplicity, each associated with furniture recommendations
#maybe we can incorporate ChatGPT here to give us feedback, currently we are only using the following...
FURNITURE_RECOMMENDATIONS = {
    'living room': ['Add an area rug', 'Create a gallery wall', 'Add a feature wall', 'Add some plants'],
    'bedroom': ['Incorporate mirrors', 'Add mood lighting', 'Buy new bedding'],
    'dining room': ['Change the light fixture', 'Add accent chair', 'Add wall paneling'],
    'office': ['Add standing desk', 'Add laptop stand', 'Include natural light'],
    'kitchen': ['Install under-cabinet lighting for ambiance', 'Upgrade to stainless steel appliances', 'Opt for sleek and handleless cabinets', 'Incorporate a kitchen island with seating'],
    'bathroom': ['Install a rainfall showerhead', 'Use high-quality, moisture-resistant paint', 'Incorporate storage solutions like shelves or cabinets'],

    # Add as many as needed
}

def predict_furniture(image):
    raw_image = Image.fromarray(image.astype('uint8'), 'RGB')
    inputs = processor(raw_image, return_tensors="pt")

    out = model.generate(**inputs)
    caption = processor.decode(out[0], skip_special_tokens=True)

    # Extract room type from the generated caption
    room_type = None
    for key in FURNITURE_RECOMMENDATIONS.keys():
        if key in caption.lower():
            room_type = key
            break

    if room_type is None:
        room_type = 'unknown: Please contact our customer support'  # If the caption doesn't match any predefined room type

    # Fetch the furniture recommendations based on the determined room type
    recommendations = FURNITURE_RECOMMENDATIONS.get(room_type, [])

    # Format the recommendations as a response
    return 'Recommendations for {}: {}'.format(room_type, ', '.join(recommendations))

iface = gr.Interface(fn=predict_furniture,
                     inputs="image",
                     outputs="text",
                     title="Furniture Recommendation System",
                     description="Upload a room image to get furniture addition recommendations")

iface.launch()